{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mlops-intro",
   "metadata": {},
   "source": [
    "# MLOps Complete Pipeline: Football Superstar Prediction\n",
    "\n",
    "This notebook implements a complete MLOps pipeline using Weights & Biases (W&B) for the Football Superstar Prediction project.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Phase 1**: Data Versioning - Version datasets as W&B artifacts\n",
    "2. **Phase 2**: Experiment Tracking - Track training runs with metrics\n",
    "3. **Phase 3**: Hyperparameter Optimization - Use W&B Sweeps\n",
    "4. **Phase 4**: Model Registration - Register best model\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install and login to W&B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install W&B if not already installed\n",
    "# !pip install wandb\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, log_loss, confusion_matrix, classification_report\n",
    )\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration - Import from centralized config\n",
    "from mlops_config import ENTITY, PROJECT, DATA_PATH, OUTPUT_DIR\n",
    "\n",
    "# Convert string paths to Path objects\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "\n",
    "# Login to W&B (only needed once per machine)\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1",
   "metadata": {},
   "source": [
    "## Phase 1: Data Versioning\n",
    "\n",
    "Version the dataset as a W&B artifact for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-versioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B run for data preparation\n",
    "run = wandb.init(\n",
    "    project=PROJECT,\n",
    "    entity=ENTITY,\n",
    "    job_type=\"data-preparation\",\n",
    "    notes=\"Cleaned and versioned Football Player dataset with temporal splits (FIFA 17-21).\"\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFIFA version distribution:\")\n",
    "print(df['fifa_version'].value_counts().sort_index())\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['big_potential'].value_counts())\n",
    "\n",
    "# Define temporal splits\n",
    "train_versions = [17.0, 18.0, 19.0, 20.0]\n",
    "val_version = 21.0\n",
    "test_version = 21.0\n",
    "\n",
    "# Split data by FIFA version\n",
    "df_train = df[df['fifa_version'].isin(train_versions)].copy()\n",
    "df_val = df[df['fifa_version'] == val_version].copy()\n",
    "df_test = df[df['fifa_version'] == test_version].copy()\n",
    "\n",
    "print(f\"\\nTrain (FIFA 17-20): {len(df_train)} samples\")\n",
    "print(f\"Validation (FIFA 21): {len(df_val)} samples\")\n",
    "print(f\"Test (FIFA 21): {len(df_test)} samples\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"football_data_prepared\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save splits\n",
    "df_train.to_csv(output_dir / \"train.csv\", index=False)\n",
    "df_val.to_csv(output_dir / \"val.csv\", index=False)\n",
    "df_test.to_csv(output_dir / \"test.csv\", index=False)\n",
    "\n",
    "# Calculate metadata\n",
    "feature_columns = [col for col in df.columns if col not in ['fifa_version', 'big_potential']]\n",
    "\n",
    "metadata = {\n",
    "    \"source\": \"feature_engineered_data_v2.csv\",\n",
    "    \"splits\": [\"train\", \"val\", \"test\"],\n",
    "    \"train_versions\": train_versions,\n",
    "    \"val_version\": val_version,\n",
    "    \"test_version\": test_version,\n",
    "    \"num_features\": len(feature_columns),\n",
    "    \"features\": feature_columns,\n",
    "    \"train_size\": len(df_train),\n",
    "    \"val_size\": len(df_val),\n",
    "    \"test_size\": len(df_test),\n",
    "    \"train_class_distribution\": {\n",
    "        \"class_0\": int((df_train['big_potential'] == 0).sum()),\n",
    "        \"class_1\": int((df_train['big_potential'] == 1).sum())\n",
    "    },\n",
    "    \"val_class_distribution\": {\n",
    "        \"class_0\": int((df_val['big_potential'] == 0).sum()),\n",
    "        \"class_1\": int((df_val['big_potential'] == 1).sum())\n",
    "    },\n",
    "    \"test_class_distribution\": {\n",
    "        \"class_0\": int((df_test['big_potential'] == 0).sum()),\n",
    "        \"class_1\": int((df_test['big_potential'] == 1).sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create W&B artifact\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"football-player-dataset\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Football player dataset with temporal splits (FIFA 17-21). Features engineered for big potential prediction.\",\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "# Add prepared data directory to artifact\n",
    "artifact.add_dir(str(output_dir))\n",
    "\n",
    "# Log artifact to W&B\n",
    "run.log_artifact(artifact)\n",
    "run.finish()\n",
    "\n",
    "print(\"\\n✅ Dataset artifact logged to W&B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2",
   "metadata": {},
   "source": [
    "## Phase 2: Experiment Tracking\n",
    "\n",
    "Train a model with experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-tracked",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B run for training\n",
    "run = wandb.init(\n",
    "    project=PROJECT,\n",
    "    entity=ENTITY,\n",
    "    job_type=\"training\",\n",
    "    config={\n",
    "        \"n_estimators\": 250,\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"subsample\": 0.7,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"reg_alpha\": 0.5,\n",
    "        \"reg_lambda\": 2.0,\n",
    "        \"scale_pos_weight\": 3,\n",
    "        \"labeled_ratio\": 0.2,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Load versioned dataset from W&B\n",
    "print(\"Loading dataset from W&B artifact...\")\n",
    "artifact = run.use_artifact(f'{ENTITY}/{PROJECT}/football-player-dataset:latest')\n",
    "data_dir = Path(artifact.download())\n",
    "\n",
    "train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "val_df = pd.read_csv(data_dir / \"val.csv\")\n",
    "test_df = pd.read_csv(data_dir / \"test.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in train_df.columns if col not in ['fifa_version', 'big_potential']]\n",
    "target = 'big_potential'\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df[target]\n",
    "X_val = val_df[feature_columns]\n",
    "y_val = val_df[target]\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Create semi-supervised split\n",
    "X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=1 - config.labeled_ratio,\n",
    "    stratify=y_train,\n",
    "    random_state=config.random_state\n",
    ")\n",
    "\n",
    "y_unlabeled = np.full(len(y_unlabeled_true), -1)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Combine for self-training\n",
    "X_train_ssl = np.vstack([X_labeled_scaled, X_unlabeled_scaled])\n",
    "y_train_ssl = np.concatenate([y_labeled.values, y_unlabeled])\n",
    "\n",
    "# Create XGBoost base estimator\n",
    "xgb_base = XGBClassifier(\n",
    "    n_estimators=config.n_estimators,\n",
    "    max_depth=config.max_depth,\n",
    "    learning_rate=config.learning_rate,\n",
    "    subsample=config.subsample,\n",
    "    colsample_bytree=config.colsample_bytree,\n",
    "    reg_alpha=config.reg_alpha,\n",
    "    reg_lambda=config.reg_lambda,\n",
    "    scale_pos_weight=config.scale_pos_weight,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=config.random_state,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create self-training classifier\n",
    "self_training = SelfTrainingClassifier(\n",
    "    xgb_base,\n",
    "    threshold=0.9,\n",
    "    criterion='threshold',\n",
    "    k_best=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining self-training model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "self_training.fit(X_train_ssl, y_train_ssl)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "wandb.log({\"training_time\": training_time})\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = self_training.predict(X_val_scaled)\n",
    "y_val_proba = self_training.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "val_log_loss = log_loss(y_val, y_val_proba)\n",
    "\n",
    "# Log validation metrics\n",
    "wandb.log({\n",
    "    \"val/accuracy\": val_accuracy,\n",
    "    \"val/precision\": val_precision,\n",
    "    \"val/recall\": val_recall,\n",
    "    \"val/f1\": val_f1,\n",
    "    \"val/roc_auc\": val_roc_auc,\n",
    "    \"val/log_loss\": val_log_loss\n",
    "})\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"F1 Score:  {val_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {val_roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = self_training.predict(X_test_scaled)\n",
    "y_test_proba = self_training.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Log test metrics\n",
    "wandb.log({\n",
    "    \"test/f1\": test_f1,\n",
    "    \"test/roc_auc\": test_roc_auc\n",
    "})\n",
    "\n",
    "run.finish()\n",
    "print(\"\\n✅ Training completed and tracked in W&B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3",
   "metadata": {},
   "source": [
    "## Phase 3: Hyperparameter Optimization\n",
    "\n",
    "Set up and run a hyperparameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val/f1',  # Optimize for F1 score\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'n_estimators': {'min': 100, 'max': 300, 'distribution': 'int'},\n",
    "        'max_depth': {'min': 3, 'max': 7, 'distribution': 'int'},\n",
    "        'learning_rate': {'min': 0.01, 'max': 0.1, 'distribution': 'uniform'},\n",
    "        'subsample': {'min': 0.6, 'max': 0.9, 'distribution': 'uniform'},\n",
    "        'colsample_bytree': {'min': 0.6, 'max': 0.9, 'distribution': 'uniform'},\n",
    "        'reg_alpha': {'min': 0.1, 'max': 1.0, 'distribution': 'uniform'},\n",
    "        'reg_lambda': {'min': 1.0, 'max': 3.0, 'distribution': 'uniform'},\n",
    "        'scale_pos_weight': {'min': 2, 'max': 4, 'distribution': 'uniform'},\n",
    "        'labeled_ratio': {'values': [0.1, 0.2, 0.3]},\n",
    "        'random_state': {'value': 42}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sweep configuration:\")\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    \"\"\"Training function for sweep.\"\"\"\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Load dataset\n",
    "    artifact = run.use_artifact(f'{ENTITY}/{PROJECT}/football-player-dataset:latest')\n",
    "    data_dir = Path(artifact.download())\n",
    "    \n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    val_df = pd.read_csv(data_dir / \"val.csv\")\n",
    "    \n",
    "    feature_columns = [col for col in train_df.columns if col not in ['fifa_version', 'big_potential']]\n",
    "    X_train = train_df[feature_columns]\n",
    "    y_train = train_df['big_potential']\n",
    "    X_val = val_df[feature_columns]\n",
    "    y_val = val_df['big_potential']\n",
    "    \n",
    "    # Semi-supervised split\n",
    "    X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(\n",
    "        X_train, y_train, test_size=1 - wandb.config.labeled_ratio,\n",
    "        stratify=y_train, random_state=wandb.config.random_state\n",
    "    )\n",
    "    y_unlabeled = np.full(len(y_unlabeled_true), -1)\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "    X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    X_train_ssl = np.vstack([X_labeled_scaled, X_unlabeled_scaled])\n",
    "    y_train_ssl = np.concatenate([y_labeled.values, y_unlabeled])\n",
    "    \n",
    "    # Train\n",
    "    xgb_base = XGBClassifier(\n",
    "        n_estimators=int(wandb.config.n_estimators),\n",
    "        max_depth=int(wandb.config.max_depth),\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        subsample=wandb.config.subsample,\n",
    "        colsample_bytree=wandb.config.colsample_bytree,\n",
    "        reg_alpha=wandb.config.reg_alpha,\n",
    "        reg_lambda=wandb.config.reg_lambda,\n",
    "        scale_pos_weight=wandb.config.scale_pos_weight,\n",
    "        eval_metric=\"logloss\", tree_method=\"hist\",\n",
    "        random_state=wandb.config.random_state, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    self_training = SelfTrainingClassifier(xgb_base, threshold=0.9, verbose=False)\n",
    "    self_training.fit(X_train_ssl, y_train_ssl)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_val_pred = self_training.predict(X_val_scaled)\n",
    "    y_val_proba = self_training.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    \n",
    "    wandb.log({\"val/f1\": val_f1, \"val/roc_auc\": val_roc_auc})\n",
    "    \n",
    "    run.finish()\n",
    "\n",
    "# Create and run sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT, entity=ENTITY)\n",
    "print(f\"Created sweep: {sweep_id}\")\n",
    "print(f\"URL: https://wandb.ai/{ENTITY}/{PROJECT}/sweeps/{sweep_id}\")\n",
    "\n",
    "# Run sweep (adjust count as needed)\n",
    "# wandb.agent(sweep_id, train_sweep, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4",
   "metadata": {},
   "source": [
    "## Phase 4: Model Registration\n",
    "\n",
    "Register the best model from the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run from sweep\n",
    "api = wandb.Api()\n",
    "# Replace with your actual sweep_id\n",
    "# sweep_id = \"your-sweep-id\"\n",
    "# sweep = api.sweep(f\"{ENTITY}/{PROJECT}/{sweep_id}\")\n",
    "# best_run = sweep.best_run()\n",
    "\n",
    "# print(f\"Best run: {best_run.name}\")\n",
    "# print(f\"Best config: {best_run.config}\")\n",
    "# print(f\"Best F1: {best_run.summary.get('val/f1', 'N/A')}\")\n",
    "\n",
    "# Then retrain with best config and register as artifact\n",
    "# (See mlops_model_registry.py for complete implementation)\n",
    "\n",
    "print(\"See mlops_model_registry.py for complete model registration code.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
